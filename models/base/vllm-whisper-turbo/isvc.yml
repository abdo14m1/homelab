apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: whisper-turbo
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    runtimeClassName: nvidia
    containers:
      - name: kserve-container
        image: vllm/vllm-openai:v0.6.4.post1
        ports:
          - containerPort: 8080
            protocol: TCP
        env:
          - name: PORT
            value: "8080"
          - name: VLLM_ATTENTION_BACKEND
            value: "xformers"
        args:
          - "--model"
          - "/mnt/models/whisper-large-v3-turbo"
          - "--task"
          - "transcription"
          - "--trust-remote-code"
          - "--gpu-memory-utilization"
          - "0.95"
          - "--dtype"
          - "float16"
          - "--max-model-len"
          - "448"
        resources:
          limits:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "2"
            memory: "8Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
          - name: model-volume
            mountPath: /mnt/models
            readOnly: true
    volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: whisper-model-pvc