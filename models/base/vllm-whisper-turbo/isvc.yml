apiVersion: "serving.kserve.io/v1beta1"
kind: InferenceService
metadata:
  name: whisper-turbo
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    containers:
      - name: kserve-container
        image: vllm/vllm-openai:nightly-7c572544e4292cb97e2f4f5a68c43f02406ad1d6
        args:
          - "--host=0.0.0.0"
          - "--port=8080"
          - "--model=openai/whisper-large-v3-turbo"
          - "--engine-use-ray"
          - "--trust-remote-code"
        env:
          - name: HF_HOME
            value: /mnt/models
        volumeMounts:
          - name: model-cache-storage
            mountPath: /mnt/models
        ports:
          - containerPort: 8080
            protocol: TCP
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
    volumes:
      - name: model-cache-storage
        persistentVolumeClaim:
          claimName: model-cache-pvc