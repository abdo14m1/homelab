apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: whisper-turbo
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    runtimeClassName: nvidia
    containers:
      - name: kserve-container
        image: fedirz/faster-whisper-server:cuda
        ports:
          - containerPort: 8000
            protocol: TCP
        env:
          - name: PORT
            value: "8000"
          # This will now successfully download to the freed-up PVC
          - name: WHISPER_MODEL
            value: "deepdml/faster-whisper-large-v3-turbo-ct2"
          - name: WHISPER_COMPUTE_TYPE
            value: "int8"
          - name: WHISPER_DEVICE
            value: "cuda"
        resources:
          limits:
            cpu: "2"
            memory: "6Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: "4Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
          - name: model-volume
            mountPath: /root/.cache/huggingface
    volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: whisper-model-pvc