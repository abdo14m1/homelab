apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: whisper-turbo
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    runtimeClassName: nvidia
    containers:
      - name: kserve-container
        image: fedirz/faster-whisper-server:sha-307e23f-cuda
        ports:
          - containerPort: 8000
            protocol: TCP
        env:
          - name: PORT
            value: "8000"
          - name: WHISPER__MODEL
            value: "Systran/faster-whisper-large-v3"
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: hf_token
        resources:
          limits:
            cpu: "2"
            memory: "6Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: "4Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
          - name: model-volume
            mountPath: /home/ubuntu/.cache/huggingface
    volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: whisper-model-pvc